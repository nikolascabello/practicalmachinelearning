---
title: "Theming with bslib and thematic"
author: "Nicolas cabello"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

The initial steps involve loading necessary libraries and data
(pml.training and pml.testing). The data is then cleaned by removing
columns with near-zero variance and missing values, ensuring that the
training data is ready for model

```{r}
library(caret)
library(corrplot)
library(tidyverse)
library(randomForest)
library(kernlab)
library(rattle)
library(gbm) 

```

```{r}
#con varianza cercana a 0  NAS fuera

testing<- read.csv("pml-testing.csv")
training<- read.csv("pml-training.csv")
training <- training[,-c(1:7)] 
training <- training[, colSums(is.na(training)) == 0]
depured0<- nearZeroVar(training)
trainingd0<-training[,-depured0]
```

After preprocessing, the dataset is split into training and validation
sets using stratified sampling. This is done to ensure that each class
is represented proportionally in both sets.

```{r}
#ok now the split
set.seed(1542) 
traiN <- createDataPartition( trainingd0$classe,
p=0.75,list = F)
trainingR<-trainingd0[traiN,]
validationR<-trainingd0[-traiN,]
```

The first model built is a Random Forest classifier. Cross-validation
(CV) is performed with 5 folds using the trainControl function, and the
model is trained using the train function with the 'rf' method. The
number of trees is set to 50.

```{r}
#modeling
library(caret)
library(corrplot)
library(tidyverse)
library(randomForest)
library(kernlab)
library(rattle)
library(gbm) 
rfcv <- trainControl(method = "cv", number = 5) 
rfcvmodel <- train(classe ~.,data =trainingR, method = "rf", trControl = rfcv,tuneLength =5,ntree=50) 
print(rfcvmodel)
test_predictions <- predict(rfcvmodel, validationR)
cmrfcv <- confusionMatrix(test_predictions,
as.factor(validationR$classe))

cmrfcv 
```

A Decision Tree classifier is built next. The same CV settings are used
as with Random Forest, and the 'rpart' method is employed.

```{r}
#arbol de decision 
library(caret)
library(corrplot)
library(tidyverse)
library(randomForest)
library(kernlab)
library(rattle)
library(gbm) 
rpartcvmodel <- train(classe~., data=trainingR,method="rpart", trControl = rfcv, tuneLength = 10)
fancyRpartPlot(rpartcvmodel$finalModel) 
rpartcvmodel

test_predictions <- predict(rpartcvmodel, validationR)
cmrpartcv <-confusionMatrix(test_predictions, as.factor(validationR$classe))
cmrpartcv 
```

A Gradient Boosted Machine model is also trained using the same CV
settings as the previous models.

```{r}

#gradient boosted 
library(caret)
library(corrplot)
library(tidyverse)
library(randomForest)
library(kernlab)
library(rattle)
library(gbm) 
gbmcvmodel <- train(classe~., data=trainingR, method="gbm", trControl = rfcv,
tuneLength = 4, verbose = F) 
test_predictions <- predict(gbmcvmodel,
validationR) 
cmcvgbm <- confusionMatrix(test_predictions,
as.factor(validationR$classe))

cmcvgbm #cmcvgbm Confusion Matrix and Statistics
```

The models' accuracy values are compared, and the Random Forest model is
found to be the most accurate among the three.

```{r}

# Print the acuracy
cat("Accuracy:",
cmrfcv$overall['Accuracy'], "RF", cmrpartcv$overall['Accuracy'],"rpart",
cmcvgbm$overall['Accuracy'], "gbm")
#Accuracy: 0.9942904 RF 0.6853589 rpart 0.9818515 gbm\> 
#random forest more accurate
```

testeando el set

```{r}

predictionsforproject<- predict(rfcvmodel,testing)
predictionsforproject 
plot(rfcvmodel) 
# results B A B A A E D B A A B C B A E E A B B B
```

out of sample error it can be calculated as the dataset doesnt contain
the correct classes
